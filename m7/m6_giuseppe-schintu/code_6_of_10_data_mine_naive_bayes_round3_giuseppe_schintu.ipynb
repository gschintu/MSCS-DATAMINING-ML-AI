{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/24 17:01:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.ml.feature import StopWordsRemover, Tokenizer\n",
    "from pyspark.ml.feature import CountVectorizer, IDF\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    ".appName('code_6_of_10_data_mine_giuseppe_schintu') \\\n",
    ".master('local[*]') \\\n",
    ".config('spark.sql.execution.arrow.pyspark.enabled', True) \\\n",
    ".config('spark.sql.session.timeZone', 'UTC') \\\n",
    ".config('spark.driver.memory','8G') \\\n",
    ".config('spark.ui.showConsoleProgress', True) \\\n",
    ".config('spark.sql.repl.eagerEval.enabled', True) \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### => user-defined functions (lemmatize, classify_sentence, author_fullname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.author_fullname(author)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize sentence (lemmatize words, remove stop words and punctuations, strip off html and digits)\n",
    "# Returns a struct of tokens, tokens count, punctuation count\n",
    "\n",
    "import string\n",
    "import unicodedata\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructField, StructType, ArrayType, IntegerType, StringType\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "list_punct = set(string.punctuation)\n",
    "url_pattern = re.compile(r'https?.+|[^(a-zA-Z)(0-9)\\s]')\n",
    "number_pattern = re.compile(r'\\d+')\n",
    "\n",
    "def lemmatize(text):\n",
    "    \"\"\"\n",
    "    param: sentence\n",
    "    return: tokens, tokens count, punctuation count\n",
    "    \"\"\"\n",
    "    punctuation_table = str.maketrans('', '', string.punctuation)\n",
    "    \n",
    "    punct_count = text.translate(punctuation_table).count('')\n",
    "    \n",
    "    text = (unicodedata.normalize('NFKD', text)\n",
    "            .encode('ascii', 'ignore')\n",
    "            .decode('utf-8', 'ignore')\n",
    "            .lower())\n",
    "    \n",
    "    # remove urls\n",
    "    text = url_pattern.sub(' ', text)\n",
    "    # remove numbers\n",
    "    text = number_pattern.sub(' ', text)\n",
    "    \n",
    "    words = text.split()\n",
    "    # remove stopwords and strings of length <= 2\n",
    "    words = [wnl.lemmatize(word) for word in words if word not in stopwords_set and len(word) > 2]\n",
    "    word_count = len(words)\n",
    "    \n",
    "    return words, word_count, punct_count\n",
    "\n",
    "\n",
    "# Register lemmatizer as an UDF\n",
    "lemma_schema = StructType([\n",
    "    StructField(\"words\", ArrayType(StringType()), False),\n",
    "    StructField(\"word_count\", IntegerType(), False),\n",
    "    StructField(\"punct_count\", IntegerType(), False)\n",
    "])\n",
    "\n",
    "udf_lemmatize = F.udf(lemmatize, lemma_schema)\n",
    "\n",
    "#Create function and register in SQL for further use\n",
    "def classify_sentence(words):\n",
    "    if words > 30:\n",
    "        return \"Wordy\"\n",
    "    elif words < 7:\n",
    "        return \"Pity\"\n",
    "    elif words <= 30 and words >=7:\n",
    "        return \"Not Wordy\"\n",
    "\n",
    "spark.udf.register(\"classify_sentence\", classify_sentence,StringType())\n",
    "\n",
    "def author_fullname(author):\n",
    "    if author == \"EAP\":\n",
    "        return \"Edgar Allan Poe\"\n",
    "    elif author == \"HPL\":\n",
    "        return \"H.P. Lovecraft\"\n",
    "    elif author == \"MWS\":\n",
    "        return \"Mary Shelley\"\n",
    "\n",
    "spark.udf.register(\"author_fullname\", author_fullname,StringType())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# => `ASSIGNMENT 5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (No Hyperparameters): 83.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with CV and ParamGrid: 84.57%\n"
     ]
    }
   ],
   "source": [
    "#may use in other places\n",
    "rnd_seed = 42\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StopWordsRemover, Tokenizer, CountVectorizer, IDF\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.functions import concat_ws\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import (\n",
    "    col, \n",
    "    count,\n",
    "    regexp_replace,\n",
    "    row_number,\n",
    "    udf,\n",
    ")\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "# Set the logging level to ERROR\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Read train and test data files into Spark DataFrames\n",
    "train_data_path = \"train.csv\"\n",
    "#test_data_path = \"\"\n",
    "\n",
    "train_data = spark.read.csv(train_data_path, header=True, inferSchema=True, escape='\"', sep=\",\")\n",
    "#test_data = spark.read.csv(test_data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "rnd_seed = 42\n",
    "\n",
    "# Use the UDF to extract the tokens and count features\n",
    "lemma_train_df = (train_data\n",
    " .withColumn('lemmatize', udf_lemmatize('text'))\n",
    ")\n",
    "\n",
    "lemma_train_df.cache()\n",
    "\n",
    "lemma_train_df = lemma_train_df.select(F.col(\"id\"),\n",
    "                    F.col(\"text\"),\n",
    "                    F.col(\"author\"), \n",
    "                    F.col(\"lemmatize.words\").alias(\"words\"),\n",
    "                    F.col(\"lemmatize.word_count\").alias(\"word_count\"),\n",
    "                    F.col(\"lemmatize.punct_count\").alias(\"punct_count\")\n",
    "                   ).cache()\n",
    "\n",
    "lemma_train_df = lemma_train_df.withColumn(\"words_str\", concat_ws(\" \", lemma_train_df.words))\n",
    "\n",
    "\n",
    "#test\n",
    "'''\n",
    "#Word2Vec does not work in this scenario as it returns negative values, and it is incompatible with NaiveBayes. But, we tried...\n",
    "# Define text preprocessing pipeline\n",
    "tokenizer = Tokenizer(inputCol=\"words_str\", outputCol=\"tokens\")\n",
    "lemma_train_df = tokenizer.transform(lemma_train_df)\n",
    "\n",
    "stopwords_remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"filtered_tokens\")\n",
    "lemma_train_df = stopwords_remover.transform(lemma_train_df)\n",
    "\n",
    "word2vec = Word2Vec(vectorSize=500, minCount=0, inputCol=\"filtered_tokens\", outputCol=\"features\")\n",
    "model = word2vec.fit(lemma_train_df)\n",
    "lemma_train_df = model.transform(lemma_train_df)\n",
    "\n",
    "lemma_train_df.show()\n",
    "'''\n",
    "\n",
    "#test end\n",
    "\n",
    "\n",
    "\n",
    "# Define text preprocessing pipeline\n",
    "tokenizer = Tokenizer(inputCol=\"words_str\", outputCol=\"tokens\")\n",
    "stopwords_remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"filtered_tokens\")\n",
    "\n",
    "# TF-IDF vectorization\n",
    "cv = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"raw_features\")\n",
    "idf = IDF(inputCol=cv.getOutputCol(), outputCol=\"features_idf\")\n",
    "normalizer = Normalizer(inputCol=idf.getOutputCol(), outputCol=\"features\")\n",
    "label_indexer = StringIndexer(inputCol = \"author\", outputCol = \"label\").setHandleInvalid(\"keep\")\n",
    "\n",
    "preprocessing_pipeline = Pipeline(stages=[tokenizer, stopwords_remover, cv, idf, normalizer, label_indexer])\n",
    "\n",
    "# Apply text preprocessing pipeline on train and test data\n",
    "preprocessed_train_data = preprocessing_pipeline.fit(lemma_train_df).transform(lemma_train_df)\n",
    "\n",
    "\n",
    "# Convert author_label from float to int\n",
    "#processed_data = preprocessed_train_data.withColumn(\"label\", col(\"label\").cast(\"integer\"))\n",
    "\n",
    "#Balance data technique ..\n",
    "'''\n",
    "# Add an ID column to the data\n",
    "windowSpec = Window.partitionBy(\"author\").orderBy(\"id\")\n",
    "processed_data_with_id = preprocessed_train_data.withColumn(\"id\", row_number().over(windowSpec))\n",
    "\n",
    "# Group the data by author and get the count of each author\n",
    "author_counts = processed_data_with_id.groupBy(\"author\").count()\n",
    "total_test_proportion = 0.20\n",
    "test_sizes = author_counts.select(\"author\", (col(\"count\") * total_test_proportion).alias(\"holdout\"))\n",
    "data_with_test_sizes = processed_data_with_id.join(test_sizes, \"author\")\n",
    "\n",
    "# Split the data into train and test based on the calculated test sizes\n",
    "preprocessed_train_data = data_with_test_sizes.filter(col(\"id\") > col(\"holdout\")).drop(\"holdout\", \"id\")\n",
    "preprocessed_test_data = data_with_test_sizes.filter(col(\"id\") <= col(\"holdout\")).drop(\"holdout\", \"id\")\n",
    "preprocessed_train_data = preprocessed_train_data.withColumn(\"label\", col(\"label\").cast(\"double\"))\n",
    "preprocessed_test_data = preprocessed_test_data.withColumn(\"label\", col(\"label\").cast(\"double\"))\n",
    "print(\"Author counts in the train set:\")\n",
    "preprocessed_train_data.groupBy(\"author\").count().show()\n",
    "print(\"Author counts in the test set:\")\n",
    "preprocessed_test_data.groupBy(\"author\").count().show()\n",
    "\n",
    "'''\n",
    "#random split seems to work better the commented out balance Data Technique\n",
    "preprocessed_train_data, preprocessed_test_data = preprocessed_train_data.randomSplit([0.90, 0.10], seed=42)\n",
    "\n",
    "\n",
    "# Prepare data for Naive Bayes classification\n",
    "train_dataset = preprocessed_train_data.select(\"features\", \"label\")\n",
    "test_dataset = preprocessed_test_data.select(\"features\", \"label\")\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "\n",
    "# Train Naive Bayes model\n",
    "naive_bayes = NaiveBayes()\n",
    "naive_bayes_model = naive_bayes.fit(train_dataset)\n",
    "\n",
    "# Make predictions on test data using Naive Bayes model\n",
    "predictions = naive_bayes_model.transform(test_dataset)\n",
    "\n",
    "# Evaluate the predictions\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy (No Hyperparameters): {:.2%}\".format(accuracy))\n",
    "\n",
    "#hyperparameters\n",
    "\n",
    "# Use ParamGridBuilder to construct a grid of parameters to search over\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(naive_bayes.smoothing, [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "# Define cross-validation\n",
    "crossval = CrossValidator(estimator=naive_bayes,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(),\n",
    "                          numFolds=3)  # use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cv_model = crossval.fit(train_dataset)\n",
    "\n",
    "# Make predictions on test data using the model with the best set of parameters.\n",
    "cv_predictions = cv_model.transform(test_dataset)\n",
    "\n",
    "# Evaluate the model\n",
    "cv_accuracy = evaluator.evaluate(cv_predictions)\n",
    "print(\"Accuracy with CV and ParamGrid: {:.2%}\".format(cv_accuracy))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
